# -*- coding: utf-8 -*-
"""ML_Week1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FMm4YEDe3h7RMEi4lTyGKeTrveGv4MMV
"""

import numpy as np
import pandas as pd

df=pd.read_csv("week1.csv", names=["Target","value"]) 
print(df)

X=np.array(df.iloc[:,0])
X=X.reshape(-1, 1)
y=np.array(df.iloc[:,1])
y=y.reshape(-1, 1)

normalized_df=(df-df.mean())/df.std()
print(normalized_df)

normalized_df=(df.Target-df.Target.min())/(df.Target.max()-df.Target.min())
print(normalized_df)

import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (12.0, 9.0)
X = normalized_df
Y = df.iloc[:, 1]
plt.scatter(X, Y)
plt.xlabel('Input')
plt.ylabel('Target')
plt.show()

from copy import deepcopy
m = 0
c = 0

L = 0.001  # The learning Rate
epochs = 10000  # The number of iterations to perform gradient descent

n = float(len(X)) # Number of elements in X

print(n)
loss=list()

def cost_fn(pre,y):
  m=len(y)
  y = y.reshape(y.shape[0],)
  cost=(1/2*m)*np.sum(np.square(pre-y))
  return cost

for i in range(epochs): 
    Y_pred = m*X + c  
    loss.append((i,cost_fn(Y_pred,y)))
    D_m = (-2/n) * sum(X * (Y - Y_pred)) 
    D_c = (-2/n) * sum(Y - Y_pred)  
    m = m - L * D_m  
    c = c - L * D_c  
    if i%100==0:
          print(i,m, c)

print(loss[-1][1])        
print (m, c)

xAxis, yAxis = zip(*loss)
plt.plot(xAxis,yAxis)
plt.title('Learning Rate')
plt.xlabel('Number of epochs')
plt.ylabel('Loss (Cost Function)')
plt.show()

Y_pred = m*X + c
plt.scatter(X, Y) 
plt.plot([min(X), max(X)], [min(Y_pred), max(Y_pred)], color='red')  # regression line
plt.title('Linear Regression Graph')
plt.xlabel('Input')
plt.ylabel('Target')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
from scipy import stats
from datetime import datetime
from sklearn import preprocessing
from sklearn.model_selection import KFold
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
# %matplotlib inline
normalized_df.head()

df.shape
df.plot(figsize=(18,5))

X = pd.DataFrame(df['Target'])
y = pd.DataFrame(df['value'])
model = LinearRegression()
scores = []

kfold = KFold(n_splits=3, shuffle=True, random_state=42)
for i, (train, test) in enumerate(kfold.split(X, y)):
  model.fit(X.iloc[train,:], y.iloc[train,:])
  score = model.score(X.iloc[test,:], y.iloc[test,:])
  scores.append(score)
print(scores)