# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uMHSgqMydSvXFMSPiyGpKs1t-gRCYLeW

### Libraries
"""

pip install json_lines

# Commented out IPython magic to ensure Python compatibility.
import json_lines 
import re
import string
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
import pandas
import pandas as pd
import numpy as np
from sklearn import metrics, svm
import seaborn as sn
from sklearn.linear_model import Lasso
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
# %matplotlib inline

"""### Data Processing and Cleaning """

X=[]; y=[]; z=[]
with open('reviews_111.jl', 'rb') as f: 
  for item in json_lines.reader(f):
    X.append(item['text']) 
    y.append(item['voted_up']) 
    z.append(item['early_access'])

data = {'Review': X, 'Voted_Up': y, 'Early_Access':z}
df = pandas.DataFrame(data=data)
print(df)

def cleaning(text):
  text=text.lower()
  text=re.sub('\[.*?\],!','', text)
  text=re.sub('[%s]'% re.escape(string.punctuation),'',text)
  text=re.sub('\w*\d\w*','',text)
  return text

cleaned = lambda x: cleaning(x)

df['Review']=pandas.DataFrame(df.Review.apply(cleaned))
X=np.array(df.Review)
y=np.array(df.Voted_Up)
z=np.array(df.Early_Access)
print(df)

"""### Data Visualisation """

round(df.Voted_Up.value_counts(normalize=True)*100,2).plot(kind='bar')
plt.title("Percentage of Voted Up")
plt.show()

round(df.Early_Access.value_counts(normalize=True)*100,2).plot(kind='bar')
plt.title("Percentage of Early Access")
plt.show()

cleaned_reviews=df.Review
voted_up=df.Voted_Up
early=df.Early_Access

re_train,re_test,vo_train,vo_test,ea_train,ea_test = train_test_split(cleaned_reviews,voted_up,early,test_size=0.25,random_state=255)

print("re_train: ",len(re_train))
print("re_test: ",len(re_test))
print("vo_train: ",len(vo_train))
print("vo_test: ",len(vo_test))
print("ea_train: ",len(ea_train))
print("ea_test: ",len(ea_test))

"""### Logistic Regression With Cross Validation For Question 1.1 and 1.2"""

tvec= TfidfVectorizer()
clf1= LogisticRegression(C=10, max_iter=500)
logic_1= Pipeline([('vectorizer',tvec),('classifier',clf1)])
logic_1.fit(re_train,vo_train)

predictions= logic_1.predict(re_test)

confusion_matrix = pd.crosstab(vo_test,predictions, rownames=['Actual'], colnames=['Predicted'])

sn.heatmap(confusion_matrix, annot=True)
print(confusion_matrix)

print("\nAccuracy: ", accuracy_score(vo_test,predictions))
print("Precision: ", precision_score(vo_test,predictions, average= 'weighted'))
print("Recall: ", recall_score(vo_test,predictions,average= 'weighted'))
print("F1 Score: ", f1_score(vo_test,predictions))
#print("Mean Square Error: ", mean_squared_error(vo_test,predictions))

sample=['worst game','happy with this game']
example=logic_1.predict(sample)

print("\nReview:'worst game'.  Vote Up Result:",example[0])
print("Review:'happy with this game'.  Vote Up Result:",example[1],'\n')
plt.show()

importance = clf1.coef_[0]
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
plt.bar([x for x in range(len(importance))], importance)
plt.show()

X=np.array(df.Review)
y=np.array(df.Voted_Up)
mean_array=[]; std_array=[]

Ci_range = [0.1, 0.5, 1, 5, 10, 50, 100]

for Ci in Ci_range:
  #model = LogisticRegression(C=Ci)
  clf1= LogisticRegression(C=Ci,max_iter=500)
  model= Pipeline([('vectorizer',tvec),('classifier',clf1)])
  small_array=[]

  kf = KFold(n_splits=10)

  for train, test in kf.split(X):
    model.fit(X[train], y[train])
    ypred = model.predict(X[test])
    scores = model.score(X[test],y[test])
    small_array.append(scores)

  mean_array.append(np.array(small_array).mean())
  std_array.append(np.array(small_array).std())
  print("when C = "+ str(Ci)+" Mean Accuracy = "+str(np.array(small_array).mean())+" Std = "+str(np.array(small_array).std()))

plt.errorbar(Ci_range,mean_array,yerr= std_array)
plt.xlabel('C value')
plt.ylabel('Accuracy')
plt.legend(['Standard Deviation'])
plt.title('Accuracy vs C Value')
plt.show()

tvec= TfidfVectorizer()
clf1= LogisticRegression(C=10, max_iter=500)
logic_2= Pipeline([('vectorizer',tvec),('classifier',clf1)])
logic_2.fit(re_train,ea_train)

predictions= logic_2.predict(re_test)

confusion_matrix = pd.crosstab(ea_test,predictions, rownames=['Actual'], colnames=['Predicted'])

sn.heatmap(confusion_matrix, annot=True)
print(confusion_matrix)

print("\nAccuracy: ", accuracy_score(ea_test,predictions))
print("Precision: ", precision_score(ea_test,predictions, average= 'weighted'))
print("Recall: ", recall_score(ea_test,predictions,average= 'weighted'))
print("F1 Score: ", f1_score(ea_test,predictions))

sample=['old game','new upcoming game']
example=logic_2.predict(sample)

print("\nReview:'old game'.  Early Access Result:",example[0])
print("Review:'new upcoming game'.  Early Access Result:",example[1],'\n')

plt.show()

X=np.array(df.Review)
y=np.array(df.Early_Access)
mean_array=[]; std_array=[]

Ci_range = [0.1, 0.5, 1, 5, 10, 50, 100]

for Ci in Ci_range:
  #model = LogisticRegression(C=Ci)
  clf1= LogisticRegression(C=Ci,max_iter=500)
  model= Pipeline([('vectorizer',tvec),('classifier',clf1)])
  small_array=[]

  kf = KFold(n_splits=10)

  for train, test in kf.split(X):
    model.fit(X[train], y[train])
    ypred = model.predict(X[test])
    scores = model.score(X[test],y[test])
    small_array.append(scores)

  mean_array.append(np.array(small_array).mean())
  std_array.append(np.array(small_array).std())
  print("when C = "+ str(Ci)+" Mean Accuracy = "+str(np.array(small_array).mean())+" Std = "+str(np.array(small_array).std()))

plt.errorbar(Ci_range,mean_array,yerr= std_array)
plt.xlabel('C value')
plt.ylabel('Accuracy')
plt.legend(['Standard Deviation'])
plt.title('Accuracy vs C Value')
plt.show()

"""### KNN With Cross Validation For Question 1.1 and 1.2"""

tvec= TfidfVectorizer()
clf2= KNeighborsClassifier(n_neighbors=4)
kNeighborsClassifier_1=Pipeline([('vectorizer',tvec),('classifier',clf2)])
kNeighborsClassifier_1.fit(re_train,vo_train)
y_pred=kNeighborsClassifier_1.predict(re_test)
confusion_matrix = pd.crosstab(vo_test, y_pred, rownames=['Actual'], colnames=['Predicted'])
sn.heatmap(confusion_matrix, annot=True)
print(confusion_matrix)
print('\nAccuracy: ',metrics.accuracy_score(vo_test, y_pred))
print("Precision: ", precision_score(vo_test, y_pred, average= 'weighted'))
print("Recall: ", recall_score(vo_test, y_pred,average= 'weighted'))
print("F1 Score: ", f1_score(vo_test, y_pred))

sample=['worst game','happy with this game']
example=kNeighborsClassifier_1.predict(sample)

print("\nReview:'worst game'.  Vote Up Result:",example[0])
print("Review:'happy with this game'.  Vote Up Result:",example[1],'\n')

plt.show()

X=np.array(df.Review)
y=np.array(df.Voted_Up)
mean_array=[]; std_array=[]

N = [2,3,4,6,8,10,20,50,100,200,500]

for n in N:

  clf2= KNeighborsClassifier(n_neighbors=n)
  model= Pipeline([('vectorizer',tvec),('classifier',clf2)])
  small_array=[]

  kf = KFold(n_splits=10)

  for train, test in kf.split(X):
    model.fit(X[train], y[train])
    ypred = model.predict(X[test])
    scores = model.score(X[test],y[test])
    small_array.append(scores)

  mean_array.append(np.array(small_array).mean())
  std_array.append(np.array(small_array).std())
  print("when n = "+ str(n)+" Mean Accuracy = "+str(np.array(small_array).mean())+" Std = "+str(np.array(small_array).std()))

plt.errorbar(N,mean_array,yerr= std_array)
plt.xlabel('K neighbors')
plt.ylabel('Accuracy')
plt.legend(['Standard Deviation'])
plt.title('Accuracy vs K neighbours')
plt.show()

tvec= TfidfVectorizer()
clf2= KNeighborsClassifier(n_neighbors=4)
kNeighborsClassifier_2=Pipeline([('vectorizer',tvec),('classifier',clf2)])
kNeighborsClassifier_2.fit(re_train,ea_train)
y_pred=kNeighborsClassifier_2.predict(re_test)
confusion_matrix = pd.crosstab(ea_test, y_pred, rownames=['Actual'], colnames=['Predicted'])
sn.heatmap(confusion_matrix, annot=True)
print(confusion_matrix)
print('\nAccuracy: ',accuracy_score(ea_test, y_pred))
print("Precision: ", precision_score(ea_test, y_pred, average= 'weighted'))
print("Recall: ", recall_score(ea_test, y_pred,average= 'weighted'))
print("F1 Score: ", f1_score(ea_test, y_pred))

sample=['old game','new upcoming game']
example=kNeighborsClassifier_2.predict(sample)

print("\nReview:'old game'.  Early Access Result:",example[0])
print("Review:'new upcoming game'.  Early Access Result:",example[1],'\n')
plt.show()

X=np.array(df.Review)
y=np.array(df.Early_Access)
mean_array=[]; std_array=[]

N = [2,4,6,8,10,20,50,100,200,500]

for n in N:

  clf2= KNeighborsClassifier(n_neighbors=n)
  model= Pipeline([('vectorizer',tvec),('classifier',clf2)])
  small_array=[]

  kf = KFold(n_splits=10)

  for train, test in kf.split(X):
    model.fit(X[train], y[train])
    ypred = model.predict(X[test])
    scores = model.score(X[test],y[test])
    small_array.append(scores)

  mean_array.append(np.array(small_array).mean())
  std_array.append(np.array(small_array).std())
  print("when n = "+ str(n)+" Mean Accuracy = "+str(np.array(small_array).mean())+" Std = "+str(np.array(small_array).std()))

plt.errorbar(N,mean_array,yerr= std_array)
plt.xlabel('K neighbors')
plt.ylabel('Accuracy')
plt.legend(['Standard Deviation'])
plt.title('Accuracy vs K neighbors')
plt.show()

"""### SVM With Cross Validation For Question 1.1 and 1.2

"""

tvec= TfidfVectorizer()
clf3 = svm.SVC(kernel='linear',C = 1)
svm_1=Pipeline([('vectorizer',tvec),('classifier',clf3)])
svm_1.fit(re_train,vo_train)
y_pred=svm_1.predict(re_test)
confusion_matrix = pd.crosstab(vo_test, y_pred, rownames=['Actual'], colnames=['Predicted'])
sn.heatmap(confusion_matrix, annot=True)
print(confusion_matrix)
print('\nAccuracy: ', accuracy_score(vo_test, y_pred))
print("Precision: ", precision_score(vo_test, y_pred, average= 'weighted'))
print("Recall: ", recall_score(vo_test, y_pred,average= 'weighted'))
print("F1 Score: ", f1_score(vo_test, y_pred))

sample=['worst game','happy with this game']
example=svm_1.predict(sample)

print("\nReview:'worst game'.  Vote Up Result:",example[0])
print("Review:'happy with this game'.  Vote Up Result:",example[1],'\n')

plt.show()

X=np.array(df.Review)
y=np.array(df.Voted_Up)
mean_array=[]; std_array=[]

Ci_range = [0.1, 0.5, 1, 5, 10, 50, 100]

for Ci in Ci_range:
  #model = LogisticRegression(C=Ci)
  clf3 = svm.SVC(kernel='linear',C = Ci)
  model= Pipeline([('vectorizer',tvec),('classifier',clf3)])
  small_array=[]

  kf = KFold(n_splits=10)

  for train, test in kf.split(X):
    model.fit(X[train], y[train])
    ypred = model.predict(X[test])
    scores = model.score(X[test],y[test])
    small_array.append(scores)

  mean_array.append(np.array(small_array).mean())
  std_array.append(np.array(small_array).std())
  print("when C = "+ str(Ci)+" Mean Accuracy = "+str(np.array(small_array).mean())+" Std = "+str(np.array(small_array).std()))

plt.errorbar(Ci_range,mean_array,yerr= std_array)
plt.xlabel('C value')
plt.ylabel('Accuracy')
plt.legend(['Standard Deviation'])
plt.title('Accuracy vs C Value')
plt.show()

tvec= TfidfVectorizer()
clf3 = svm.SVC(kernel='linear',C =10)
svm_2=Pipeline([('vectorizer',tvec),('classifier',clf3)])
svm_2.fit(re_train,ea_train)
y_pred=svm_2.predict(re_test)
confusion_matrix = pd.crosstab(ea_test, y_pred, rownames=['Actual'], colnames=['Predicted'])
sn.heatmap(confusion_matrix, annot=True)
print(confusion_matrix)
print('\nAccuracy: ',accuracy_score(ea_test, y_pred))
print("Precision: ", precision_score(ea_test, y_pred, average= 'weighted'))
print("Recall: ", recall_score(ea_test, y_pred,average= 'weighted'))
print("F1 Score: ", f1_score(ea_test, y_pred))

sample=['old game','new upcoming game']
example=svm_2.predict(sample)

print("\nReview:'old game'.  Early Access Result:",example[0])
print("Review:'new upcoming game'.  Early Access Result:",example[1],'\n')
plt.show()

X=np.array(df.Review)
y=np.array(df.Early_Access)
mean_array=[]; std_array=[]

Ci_range = [0.1, 0.5, 1, 5, 10, 50, 100]

for Ci in Ci_range:
  #model = LogisticRegression(C=Ci)
  clf3 = svm.SVC(kernel='linear',C = Ci)
  model= Pipeline([('vectorizer',tvec),('classifier',clf3)])
  small_array=[]

  kf = KFold(n_splits=10)

  for train, test in kf.split(X):
    model.fit(X[train], y[train])
    ypred = model.predict(X[test])
    scores = model.score(X[test],y[test])
    small_array.append(scores)

  mean_array.append(np.array(small_array).mean())
  std_array.append(np.array(small_array).std())
  print("when C = "+ str(Ci)+" Mean Accuracy = "+str(np.array(small_array).mean())+" Std = "+str(np.array(small_array).std()))

plt.errorbar(Ci_range,mean_array,yerr= std_array)
plt.xlabel('C value')
plt.ylabel('Accuracy')
plt.legend(['Standard Deviation'])
plt.title('Accuracy vs C Value')
plt.show()

"""### ROC Curve For Question 1.1 and Question 1.2"""

def baseline_model(X):
  print(np.ones(X.shape[0]))
  return np.ones(X.shape[0])

baseline_y_pred = baseline_model(re_test)

from sklearn.metrics import roc_curve,auc

Logi_y_pred = logic_1.predict(re_test)
Knn_y_pred = kNeighborsClassifier_1.predict(re_test)
svm_y_pred = svm_1.predict(re_test)
baseline_y_pred = baseline_model(re_test)

confusion_matrix2 = confusion_matrix(vo_test,Knn_y_pred)
confusion_matrix3 = confusion_matrix(vo_test,baseline_y_pred)
confusion_matrix1 = confusion_matrix(vo_test,Logi_y_pred)
confusion_matrix4 = confusion_matrix(vo_test,svm_y_pred)

tn, fp, fn, tp = confusion_matrix3.ravel()
print('For Baseline Model: Number of True Negatives: ',tn,' False Positives: ',fp,' False Negatives: ',fn,' True Positives : ',tp)
tn1, fp1, fn1, tp1 = confusion_matrix1.ravel()
print('\nFor Logistic Regression: Number of True Negatives: ',tn1,' False Positives: ',fp1,' False Negatives: ',fn1,' True Positives : ',tp1)
tn2, fp2, fn2, tp2 = confusion_matrix2.ravel()
print('\nFor Knn: Number of True Negatives: ',tn2,' False Positives: ',fp2,' False Negatives: ',fn2,' True Positives : ',tp2)
tn2, fp2, fn2, tp2 = confusion_matrix4.ravel()
print('\nFor SVM: Number of True Negatives: ',tn2,' False Positives: ',fp2,' False Negatives: ',fn2,' True Positives : ',tp2,'\n')


False_Positive_rate,True_Positive_rate,_ = roc_curve(vo_test,logic_1.decision_function(re_test))
False_Positive_rate2,True_Positive_rate2,_ = roc_curve(vo_test,kNeighborsClassifier_1.predict_proba(re_test)[:,1])
False_Positive_rate3,True_Positive_rate3,_ = roc_curve(vo_test,baseline_y_pred)
False_Positive_rate4,True_Positive_rate4,_ = roc_curve(vo_test,svm_1.decision_function(re_test))

roc_auc = auc(False_Positive_rate, True_Positive_rate)
roc_auc2 = auc(False_Positive_rate2, True_Positive_rate2)
roc_auc3 = auc(False_Positive_rate3, True_Positive_rate3)
roc_auc4 = auc(False_Positive_rate4, True_Positive_rate4)

plt.plot(False_Positive_rate,True_Positive_rate,'r', label = 'Logistic AUC = %0.4f' % roc_auc)
plt.plot(False_Positive_rate2,True_Positive_rate2,'b', label = 'KNN AUC = %0.4f' % roc_auc2)
plt.plot(False_Positive_rate3,True_Positive_rate3,linestyle='--',label = 'Baseline AUC = %0.4f' % roc_auc3)
plt.plot(False_Positive_rate4,True_Positive_rate4,'g', label = 'SVM AUC = %0.4f' % roc_auc4)
plt.legend(loc = 'lower right')
plt.rc('font', size=10); plt.rcParams['figure.constrained_layout.use'] = True
plt.xlabel('False positive rate') 
plt.ylabel('True positive rate') 
# plt.legend(['Logistic Regression','KNN Classifier', 'Baseline Model'])
plt.title('Roc Curve')

from sklearn.metrics import roc_curve,auc

Logi_y_pred = logic_2.predict(re_test)
Knn_y_pred = kNeighborsClassifier_2.predict(re_test)
svm_y_pred = svm_2.predict(re_test)
baseline_y_pred = baseline_model(re_test)

confusion_matrix2 = confusion_matrix(ea_test,Knn_y_pred)
confusion_matrix3 = confusion_matrix(ea_test,baseline_y_pred)
confusion_matrix1 = confusion_matrix(ea_test,Logi_y_pred)
confusion_matrix4 = confusion_matrix(ea_test,svm_y_pred)

tn, fp, fn, tp = confusion_matrix3.ravel()
print('For Baseline Model: Number of True Negatives: ',tn,' False Positives: ',fp,' False Negatives: ',fn,' True Positives : ',tp)
tn1, fp1, fn1, tp1 = confusion_matrix1.ravel()
print('\nFor Logistic Regression: Number of True Negatives: ',tn1,' False Positives: ',fp1,' False Negatives: ',fn1,' True Positives : ',tp1)
tn2, fp2, fn2, tp2 = confusion_matrix2.ravel()
print('\nFor Knn: Number of True Negatives: ',tn2,' False Positives: ',fp2,' False Negatives: ',fn2,' True Positives : ',tp2)
tn2, fp2, fn2, tp2 = confusion_matrix4.ravel()
print('\nFor SVM: Number of True Negatives: ',tn2,' False Positives: ',fp2,' False Negatives: ',fn2,' True Positives : ',tp2,'\n')


False_Positive_rate,True_Positive_rate,_ = roc_curve(ea_test,logic_2.decision_function(re_test))
False_Positive_rate2,True_Positive_rate2,_ = roc_curve(ea_test,kNeighborsClassifier_2.predict_proba(re_test)[:,1])
False_Positive_rate3,True_Positive_rate3,_ = roc_curve(ea_test,baseline_y_pred)
False_Positive_rate4,True_Positive_rate4,_ = roc_curve(ea_test,svm_2.decision_function(re_test))

roc_auc = auc(False_Positive_rate, True_Positive_rate)
roc_auc2 = auc(False_Positive_rate2, True_Positive_rate2)
roc_auc3 = auc(False_Positive_rate3, True_Positive_rate3)
roc_auc4 = auc(False_Positive_rate4, True_Positive_rate4)

plt.plot(False_Positive_rate,True_Positive_rate,'r', label = 'Logistic AUC = %0.4f' % roc_auc)
plt.plot(False_Positive_rate2,True_Positive_rate2,'b', label = 'KNN AUC = %0.4f' % roc_auc2)
plt.plot(False_Positive_rate3,True_Positive_rate3,linestyle='--',label = 'Baseline AUC = %0.4f' % roc_auc3)
plt.plot(False_Positive_rate4,True_Positive_rate4,'g', label = 'SVM AUC = %0.4f' % roc_auc4)
plt.legend(loc = 'lower right')
plt.rc('font', size=10); plt.rcParams['figure.constrained_layout.use'] = True
plt.xlabel('False positive rate') 
plt.ylabel('True positive rate') 
# plt.legend(['Logistic Regression','KNN Classifier', 'Baseline Model'])
plt.title('Roc Curve')