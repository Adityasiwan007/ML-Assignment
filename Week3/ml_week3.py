# -*- coding: utf-8 -*-
"""ML_Week3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EQx79B3Kqh4djqYTyzaoZScHlynVHCTc
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as pp
# %matplotlib inline
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt
import numpy as np
import sklearn.linear_model
from sklearn.model_selection import KFold 
from mpl_toolkits.mplot3d import Axes3D
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error 
from sklearn.preprocessing import PolynomialFeatures
from pandas import DataFrame
from matplotlib import pyplot
from sklearn.pipeline import Pipeline
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt 
import numpy as np

df=pd.read_csv("week3.csv", names=["X1","X2","Target"]) 
X=np.array(df.iloc[:,0:2])
y=df.iloc[:,-1]
print(df)

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=0)

num_features = list()
degress = [i for i in range(1, 6)]
for d in degress:
	trans = PolynomialFeatures(degree=d)
	data = trans.fit_transform(X)
	num_features.append(data.shape[1])
	print('Degree: %d, Features: %d' % (d, data.shape[1]))
pyplot.plot(degress, num_features)
pyplot.xlabel('Degree')
pyplot.ylabel('Features')
pyplot.show()

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
pnt3d=ax.scatter(X[:,0],X[:,1],y,c=y)
cbar=plt.colorbar(pnt3d)
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('Target')
cbar.set_label("Target Depth")
plt.show()

lasso = Lasso(alpha=.01)

lasso.fit(X_train,y_train)

print ("Lasso model:", (lasso.coef_))

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X[:,0],X[:,1],y, marker='.', color='red')
ax.set_xlabel("X1")
ax.set_ylabel("X2")
ax.set_zlabel("Target")

model = lasso

coefs = model.coef_
intercept = model.intercept_
xs = np.tile(np.arange(2), (1,1))
ys = np.tile(np.arange(2), (1,1)).T
zs = xs*coefs[0]+ys*coefs[1]+intercept
print("Equation: y = {:.2f} + {:.2f}x1 + {:.2f}x2".format(intercept, coefs[0],
                                                          coefs[1]))

ax.plot_surface(xs,ys,zs, alpha=0.5)
pnt3d=ax.scatter(X[:,0],X[:,1],y,c=y)
cbar=plt.colorbar(pnt3d)
cbar.set_label("Target Depth")
plt.show()

regression_model = LinearRegression()
regression_model.fit(X_train, y_train)

ridge = Ridge(alpha=.01)

ridge.fit(X_train,y_train)

print ("Ridge model:", (ridge.coef_))

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X[:,0],X[:,1],y, marker='.', color='red')
ax.set_xlabel("X1")
ax.set_ylabel("X2")
ax.set_zlabel("Target")

model = ridge

coefs = model.coef_
intercept = model.intercept_
xs = np.tile(np.arange(2), (1,1))
ys = np.tile(np.arange(2), (1,1)).T
zs = xs*coefs[0]+ys*coefs[1]+intercept
print("Equation: y = {:.2f} + {:.2f}x1 + {:.2f}x2".format(intercept, coefs[0],
                                                          coefs[1]))
ax.plot_surface(xs,ys,zs, alpha=0.5)
pnt3d=ax.scatter(X[:,0],X[:,1],y,c=y)
cbar=plt.colorbar(pnt3d)
cbar.set_label("Target Depth")
plt.show()

# For Lasso
mean_error=[]; std_error=[]
folds = [2, 5, 10, 25, 50, 100]
c=1
for fold in folds:
    temp=[]
    kf = KFold(n_splits=fold)
    for train, test in kf.split(X): 
        model=Lasso(alpha=1/(2*c)).fit(X[train],y[train])
        ypred = model.predict(X[test])
        mean=mean_squared_error(y[test],ypred)
        temp.append(mean)
    mean_error.append(np.array(temp).mean())
    std_error.append(np.array(temp).std()) 
plt.errorbar(folds,mean_error,yerr=std_error) 
plt.xlabel('folds'); 
plt.ylabel('Mean square error') 
plt.xlim((0,110))
plt.show()

def abline(slope, intercept):
    """Plot a line from slope and intercept"""
    axes = plt.gca()
    x_vals = X
    y_vals = intercept + slope * x_vals 
    plt.plot(x_vals, y_vals, '--')
scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error') 
print(scores)
print("Accuracy for Lasso: %0.2f (+/− %0.2f)" % (scores.mean(), scores.std()))
K_fold=10
kf = KFold(n_splits=K_fold)
for train, test in kf.split(X):
    model = lasso.fit(X[train],y[train])
    ypred = model.predict(X[test])
    mean=mean_squared_error(y[test],ypred)
    print("intercept: ", model.intercept_ ," slope: ",model.coef_," square error: ",mean)
    abline(model.coef_[0],model.intercept_)

# For Ridge
mean_error=[]; std_error=[]
folds = [2, 5, 10, 25, 50, 100]
c=1
for fold in folds:
    temp=[]
    kf = KFold(n_splits=fold)
    for train, test in kf.split(X): 
        model=Ridge(alpha=1/(2*c)).fit(X[train],y[train])
        ypred = model.predict(X[test])
        mean=mean_squared_error(y[test],ypred)
        temp.append(mean)
    mean_error.append(np.array(temp).mean())
    std_error.append(np.array(temp).std()) 
plt.errorbar(folds,mean_error,yerr=std_error) 
plt.xlabel('folds'); 
plt.ylabel('Mean square error') 
plt.xlim((0,110))
plt.show()

def abline(slope, intercept):
    axes = plt.gca()
    x_vals = X
    y_vals = intercept + slope * x_vals 
    plt.plot(x_vals, y_vals, '--')

scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error') 
print(scores)
print("Accuracy for Ridge: %0.2f (+/− %0.2f)" % (scores.mean(), scores.std()))
K_fold=10
kf = KFold(n_splits=K_fold)
for train, test in kf.split(X):
    model = ridge.fit(X[train],y[train])
    ypred = model.predict(X[test])
    mean=mean_squared_error(y[test],ypred)
    print("intercept: ", model.intercept_ ," slope: ",model.coef_," square error: ",mean)
    abline(model.coef_[0],model.intercept_)